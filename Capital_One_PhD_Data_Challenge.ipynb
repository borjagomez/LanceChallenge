{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "k6bxEYTNfm7e"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "MdtvSjRFfm7i",
    "outputId": "c455d7df-c517-4d92-accf-0606f402d77e"
   },
   "outputs": [],
   "source": [
    "def IsNotebook():\n",
    "    isnotebook = None\n",
    "    isgooglecolab = None\n",
    "    shell = None\n",
    "    \n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            isnotebook = True   # Jupyter notebook or qtconsole\n",
    "            isgooglecolab = False\n",
    "        elif shell == \"Shell\":\n",
    "            isnotebook = True   # Google Colab\n",
    "            isgooglecolab = True\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            isnotebook = False  # Terminal running IPython\n",
    "            isgooglecolab = False\n",
    "        else:\n",
    "            isnotebook = False  # Other type (?)\n",
    "            isgooglecolab = False\n",
    "    except NameError:\n",
    "        isnotebook = False      # Probably standard Python interpreter\n",
    "        isgooglecolab = False\n",
    "    return shell, isnotebook, isgooglecolab\n",
    "shell, isnotebook, isgooglecolab = IsNotebook()\n",
    "\n",
    "if isnotebook:\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(\"<style>.container { width:99% !important; }</style>\"))\n",
    "    if not isgooglecolab:\n",
    "        try: #the jedi completer takes too long to complete words\n",
    "            %config Completer.use_jedi = False\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if isgooglecolab:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "    except Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "DgkX7iGtfm7l",
    "outputId": "60db9811-1add-4918-f60a-773f192ec311",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "### General Imports ###\n",
    "import os #Making sure we're using all CPU cores for faster calculations\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(os.cpu_count())\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(os.cpu_count())\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(os.cpu_count())\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(os.cpu_count())\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(os.cpu_count())\n",
    "\n",
    "import sys #Printing version for posterity\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "try: #Allows saving and loading of variables\n",
    "    import pickle5 as pickle\n",
    "except:\n",
    "    import pickle\n",
    "try: #Printing version for posterity\n",
    "    print(\"Pickle version:\", pickle.__version__)\n",
    "except:\n",
    "    print(\"Pickle version:\", pickle.format_version)\n",
    "\n",
    "import gc #We can force garbage collection to free up RAM\n",
    "import random #Enables use of random number of random choices\n",
    "import warnings #Ability to create custom warnings, like warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "import itertools #Needed for Confusion Matrix\n",
    "\n",
    "if os.name == 'nt':\n",
    "    from winsound import Beep #Uses the computer's speakers to alert you (e.g. when training is done)\n",
    "from tqdm import tqdm #Iterations can show a progress bar (like in Training)\n",
    "from collections import Counter #Allows for frequency counting similar with R's \"table\"\n",
    "#######################\n",
    "\n",
    "\n",
    "#####################\n",
    "### Date and Time ###\n",
    "import time #Gets the current time\n",
    "import dateutil.parser #Allows for Date objects like dateutil.parser.parse(\"24/05/2021\")\n",
    "from pytz import timezone #Allows for timezones to be set. #pytz.all_timezones\n",
    "from datetime import datetime #Allows for Datetime objects like current Datetime. #datetime.fromisoformat('2021-05-24')\n",
    "#####################\n",
    "\n",
    "\n",
    "###################\n",
    "### Mathematics ###\n",
    "import numpy as np #Working with numeric arrays\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "###################\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Statistics and Machine Learning ###\n",
    "#Utility\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler #Various ways of scaling the data\n",
    "from sklearn.model_selection import train_test_split #Functions for splitting datasets\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#######################################\n",
    "\n",
    "\n",
    "##################\n",
    "### Dataframes ###\n",
    "import pandas as pd\n",
    "##################\n",
    "\n",
    "\n",
    "#############\n",
    "### Plots ###\n",
    "import matplotlib.cm as cmap #Importing colormap for plots\n",
    "import matplotlib.pyplot as plt #Allows use of Pyplot plots\n",
    "\n",
    "import seaborn as sns #Allows use of Seaborn plots\n",
    "sns.set() #Sets default plot theme\n",
    "#############\n",
    "\n",
    "\n",
    "######################\n",
    "### String or Text ###\n",
    "######################\n",
    "import json #Can encode or decode JSON string objects\n",
    "\n",
    "\n",
    "###################################\n",
    "### Files, Directories, Folders ###\n",
    "from pathlib import Path\n",
    "###################################\n",
    "\n",
    "\n",
    "################################\n",
    "### Neural Network Libraries ###\n",
    "#General\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torchsummary import summary\n",
    "if isgooglecolab:\n",
    "    !pip install torchinfo\n",
    "from torchinfo import summary #Needs +1 number before conv_input_size\n",
    "\n",
    "#Data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "#Info and configuration\n",
    "print()\n",
    "print(\"PyTorch v\" + torch.__version__)\n",
    "IS_GPU_AVAILABLE = torch.cuda.is_available()\n",
    "print(f\"CUDA device available: {IS_GPU_AVAILABLE}\")\n",
    "if (torch.cuda.is_available()):\n",
    "    print(str(torch.cuda.device_count()) + \" devices available\")\n",
    "    for n in range(torch.cuda.device_count()):\n",
    "        print(\"\\t\" + torch.cuda.get_device_name(n))\n",
    "    print(\"cuda:\", torch.cuda.current_device()) #It can give you information like the GPU is not supported\n",
    "print(\"Num threads set to:\", os.cpu_count())\n",
    "torch.set_num_threads(os.cpu_count())\n",
    "################################\n",
    "\n",
    "\n",
    "########################\n",
    "### Useful functions ###\n",
    "def ZeroANumber(Number, MaxLength, ForceMaxLength = False):\n",
    "    res = str(Number).zfill(MaxLength)\n",
    "    if ForceMaxLength: res = res[:MaxLength]\n",
    "    return res\n",
    "\n",
    "def SpaceAString(CurString, MaxLength, SpaceTheFront = True, ForceMaxLength = False, ForceRemoveFromFront = False):\n",
    "    CurLen = len(CurString)\n",
    "    Result = CurString\n",
    "    \n",
    "    if CurLen < MaxLength:\n",
    "        if SpaceTheFront:\n",
    "            Result = (\" \" * (MaxLength-CurLen)) + CurString\n",
    "        else:\n",
    "            Result = CurString + (\" \" * (MaxLength-CurLen))\n",
    "    elif CurLen > MaxLength and ForceMaxLength:\n",
    "        if ForceRemoveFromFront:\n",
    "            Result = CurString[(CurLen - MaxLength):]\n",
    "        else:\n",
    "            Result = CurString[:-(CurLen - MaxLength)]\n",
    "    return Result\n",
    "\n",
    "def SaveVariable(Variable, FileName):\n",
    "    DirName = Path(FileName).parent.absolute()\n",
    "    os.makedirs(DirName, exist_ok = True)\n",
    "\n",
    "    with open(FileName, 'wb') as io:\n",
    "        pickle.dump(Variable, io)\n",
    "    \n",
    "def LoadVariable(FileName):\n",
    "    with open(FileName, \"rb\") as io:\n",
    "        Res = pickle.load(io)\n",
    "    return Res\n",
    "\n",
    "def init_seeds(seed, ForceCudaDeterministic = False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    if ForceCudaDeterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalise = False, title = 'Confusion matrix', cmap = plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalisation can be applied by setting `normalise=True`.\n",
    "    \"\"\"\n",
    "    if normalise:\n",
    "        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalisation')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalise else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment = \"center\",\n",
    "                 color = \"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\" #To FORCE CPU\n",
    "print(\"device=\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "Ug-8fSjlfm7o"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "M0SXXH_Cfm7p",
    "outputId": "4066cd48-6d49-4e19-a191-971c64e44f4b"
   },
   "outputs": [],
   "source": [
    "path_root = f\"{os.getcwd()}\"\n",
    "path_data = f\"{path_root}/Data\"\n",
    "path_models = f\"{path_root}/Models\"\n",
    "print(path_data, \"\\n\")\n",
    "print(path_root)\n",
    "print(path_models, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Zr_fnwptfm7p"
   },
   "source": [
    "●\tProgrammatically download and load into your favorite analytical tool the transactions data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "jVaLFPI0fm7q",
    "outputId": "b5dd830c-11da-4fd5-8b5a-e88cd901b154"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{path_data}/transactions.txt\"):\n",
    "    os.makedirs(f\"{path_data}\", exist_ok = True)\n",
    "    \n",
    "    if os.name == 'nt':\n",
    "        !curl -o \"{path_data}/transactions.zip\" \"https://raw.githubusercontent.com/CapitalOneRecruiting/DS/master/transactions.zip\"\n",
    "\n",
    "    else:\n",
    "        !wget \"https://raw.githubusercontent.com/CapitalOneRecruiting/DS/master/transactions.zip\" -O \"{path_data}/transactions.zip\"\n",
    "    !unzip \"{path_data}/transactions.zip\" -d \"{path_data}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "mRcWTU5Wfm7q"
   },
   "outputs": [],
   "source": [
    "def GetDataFromJSONFile(FilePath):\n",
    "    with open(FilePath) as fileinput: #Opening the file\n",
    "        i = 0\n",
    "        tmp = []\n",
    "        for line in fileinput.readlines():\n",
    "            try:\n",
    "                tmp.append(json.loads(line)) #using JSON library to load the data\n",
    "            except Exception as exc:\n",
    "                print(f\"Exception on line {i}:\\n'{line}'\\n\\n{exc}\")\n",
    "            finally:\n",
    "                i += 1\n",
    "\n",
    "    return pd.DataFrame(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "n9UZHE-ffm7r"
   },
   "outputs": [],
   "source": [
    "XY = GetDataFromJSONFile(f\"{path_data}/transactions.txt\")\n",
    "\n",
    "XY[\"transactionDateTime\"] = pd.to_datetime(XY[\"transactionDateTime\"], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "XY[\"currentExpDay\"] = \"01/\"\n",
    "XY[\"currentExpDate\"] = XY[\"currentExpDay\"].str.cat(XY[\"currentExpDate\"], sep = \"\")\n",
    "XY[\"currentExpDate\"] = pd.to_datetime(XY[\"currentExpDate\"], format=\"%d/%m/%Y\")\n",
    "XY = XY.drop([\"currentExpDay\"], axis = 1)\n",
    "\n",
    "XY[\"accountOpenDate\"] = pd.to_datetime(XY[\"accountOpenDate\"], format=\"%Y-%m-%d\")\n",
    "XY[\"dateOfLastAddressChange\"] = pd.to_datetime(XY[\"dateOfLastAddressChange\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "XY = XY.apply(lambda x: x.astype(\"int\") if isinstance(x, bool) else x).replace('', np.nan)\n",
    "XY = XY.apply(lambda x: x.str.strip() if isinstance(x, str) else x).replace('', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "F-e7KhSjfm7r"
   },
   "source": [
    "●\tPlease describe the structure of the data. Number of records and fields in each record?\n",
    "●\tPlease provide some additional basic summary statistics for each field. Be sure to include a count of null, minimum, maximum, and unique values where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "hidden": true,
    "id": "hD24YfPTfm7r",
    "outputId": "2aca6f6b-1d23-4789-e559-179fa1d770a4"
   },
   "outputs": [],
   "source": [
    "XY[XY.columns.values[:15]].describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "hidden": true,
    "id": "hD_Y_rnAfm7s",
    "outputId": "8d5c0fea-ea05-45bc-c74a-779eb42c7d26"
   },
   "outputs": [],
   "source": [
    "XY[XY.columns.values[15:]].describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "pfqLXYPjfm7s",
    "outputId": "6c87a743-0e0a-4d2f-a3b2-a9dfdd32c825"
   },
   "outputs": [],
   "source": [
    "nObservations = XY.describe(include = 'all', datetime_is_numeric = True).iloc[0, :]\n",
    "MissingPercent = (len(XY) - nObservations.values.astype(\"int\")) / len(XY) * 100\n",
    "[print(f\"{XY.columns.values[i]}: {MissingPercent[i]:.2f}% missing\") for i in range(len(nObservations))]\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "hidden": true,
    "id": "n6E9vf0pfm7s",
    "outputId": "351e3952-523d-4efc-a178-212d16a5cc78"
   },
   "outputs": [],
   "source": [
    "#These columns are 100% missing, so let's drop them\n",
    "XY = XY.drop([\"echoBuffer\", \"merchantCity\", \"merchantState\", \"merchantZip\", \"posOnPremises\", \"recurringAuthInd\"], axis = 1)\n",
    "XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "_xFCjDVqfm7t"
   },
   "source": [
    "●\tCan you programmatically identify reversed and multi-swipe transactions?\n",
    "\n",
    "●\tWhat total number of transactions and total dollar amount do you estimate for the reversed transactions? For the multi-swipe transactions? (please consider the first transaction to be \"normal\" and exclude it from the number of transaction and dollar amount counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "OvwWOdhbfm7t",
    "outputId": "263d36b3-8556-42cb-abde-07abfe14ce30"
   },
   "outputs": [],
   "source": [
    "XY.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "hidden": true,
    "id": "usABOB9Nfm7t",
    "outputId": "6939179f-3100-41b1-b6f7-c4b9e5e5ffe2"
   },
   "outputs": [],
   "source": [
    "#This is one example of a Reversal.\n",
    "#We can clearly see that there is 1 row for the actual purchase (the first transaction that is considered \"normal\")\n",
    "#Therefore, this kind of duplicate can be programmatically caught by filtering for transactionType == 'REVERSAL'\n",
    "XY.iloc[38:40, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "Nj-lZsObfm7t",
    "outputId": "fd91e8f0-e9f8-4ba0-dcad-531a19af37c0"
   },
   "outputs": [],
   "source": [
    "ReversedTransactions = XY[XY[\"transactionType\"] == 'REVERSAL']\n",
    "print(f\"total number of REVERSAL transactions:        {len(ReversedTransactions)},     {(len(ReversedTransactions) / len(XY) * 100):.2f}%\")\n",
    "print(f\"total dollar amount of REVERSAL transactions: {ReversedTransactions['transactionAmount'].sum()}, {(ReversedTransactions['transactionAmount'].sum() / XY['transactionAmount'].sum() * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "hidden": true,
    "id": "673xSPVpfm7t",
    "outputId": "2278ca5c-09a9-411c-c273-0575fee5717d"
   },
   "outputs": [],
   "source": [
    "#To capture the duplicate purchases for multi-swipe where one was charged twice, we'll focus on:\n",
    "#  transactionType==\"PURCHASES\" and cardPresent==True and transactionAmount>0 where transactionDateTime between 2 consecutive purchases is small\n",
    "\n",
    "#It seems there are 839 duplicate transactions, which amount to 0.2% of all purchases\n",
    "XY2 = XY.loc[(XY[\"transactionAmount\"] > 0) & (XY[\"transactionType\"] == 'PURCHASE') & (XY[\"cardPresent\"] == True)][[\"transactionDateTime\", \"accountNumber\", \"customerId\", \"transactionAmount\", \"cardLast4Digits\", \"merchantName\"]].sort_values(by=['transactionDateTime'])\n",
    "\n",
    "#Creating the \"Previous_\" versions of each variable in question using the shift() function\n",
    "XY2[\"Prev_transactionDateTime\"] = XY2[\"transactionDateTime\"].shift()\n",
    "XY2[\"Prev_accountNumber\"] = XY2[\"accountNumber\"].shift()\n",
    "XY2[\"Prev_customerId\"] = XY2[\"customerId\"].shift()\n",
    "XY2[\"Prev_transactionAmount\"] = XY2[\"transactionAmount\"].shift()\n",
    "XY2[\"Prev_cardLast4Digits\"] = XY2[\"cardLast4Digits\"].shift()\n",
    "XY2[\"Prev_merchantName\"] = XY2[\"merchantName\"].shift()\n",
    "XY2 = XY2.dropna()\n",
    "\n",
    "#Creating Duplicate status as discussed above\n",
    "XY2[\"IsDuplicate\"] = ((XY2[\"transactionDateTime\"] - XY2[\"Prev_transactionDateTime\"]) / pd.Timedelta(seconds = 1) < 60) &\\\n",
    "    (XY2[\"accountNumber\"] == XY2[\"Prev_accountNumber\"]) &\\\n",
    "    (XY2[\"customerId\"] == XY2[\"Prev_customerId\"]) &\\\n",
    "    (XY2[\"transactionAmount\"] == XY2[\"Prev_transactionAmount\"]) &\\\n",
    "    (XY2[\"cardLast4Digits\"] == XY2[\"Prev_cardLast4Digits\"]) &\\\n",
    "    (XY2[\"merchantName\"] == XY2[\"Prev_merchantName\"])\n",
    "\n",
    "#Counting how many duplicates there are\n",
    "counter = Counter(XY2[\"IsDuplicate\"].values)\n",
    "print(counter)\n",
    "\n",
    "#Seeing the data we used to calculate things for a sanity test\n",
    "XY2[XY2[\"IsDuplicate\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "xrXGtLIQfm7u"
   },
   "source": [
    "●\tDid you find anything interesting about either kind of transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "7cwAxzu8fm7u"
   },
   "outputs": [],
   "source": [
    "#There seems to be another category of duplicates where the same thing is purchased from the same customer on the same day, but it's not a double-swipe as those would be in a time difference of seconds, whilst some are in minutes, hours, and so on.\n",
    "\n",
    "#The amount of reversal (2.58%) is not negligible\n",
    "#Multi-swipe transaction seem to occur more than a near 0 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "SiyKoVZwfm7u"
   },
   "source": [
    "●\tPlot a histogram of the processed amounts of each transaction, the transactionAmount column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "hidden": true,
    "id": "gVmsgcgsfm7u",
    "outputId": "d6970776-61ac-4912-e5f2-3601c2cc2580"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8, 5.5]\n",
    "print(\"transactionAmount\")\n",
    "sns.histplot(x = \"transactionAmount\", data = XY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "-N976UzQfm7u"
   },
   "source": [
    "The transaction amount seems to follow a kind of power-law distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Y4BFn9Ryfm7u"
   },
   "source": [
    "●\tReport any structure you find and any hypotheses you have about that structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 946
    },
    "hidden": true,
    "id": "SIjNuvhufm7v",
    "outputId": "df0eca2f-c5bf-4226-f21a-b46aaa927f42"
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "### Data Hyperparameters ####\n",
    "Seed = 42 #The answer to Life, the Universe, and Everything.\n",
    "CustomNAString = None\n",
    "ImputationType = None #\"Zero\" \"Univariate\" \"Multivariate\"\n",
    "batch_size = 65536\n",
    "#############################\n",
    "\n",
    "\n",
    "#############################\n",
    "### Reading From One File ###\n",
    "#Already done above, in Data exploration\n",
    "\n",
    "XYColumns = XY.columns.values\n",
    "#################\n",
    "\n",
    "\n",
    "##################\n",
    "### Imputation ###\n",
    "#==For time series: Fill NAs with direction down-up, or mean of previous and next\n",
    "if ImputationType == \"Zero\":\n",
    "    #Trying filling NAs with 0\n",
    "    X_NAs = np.nan_to_num(X_NAs)\n",
    "\n",
    "elif ImputationType == \"Univariate\":\n",
    "    #Trying Univariate Imputation\n",
    "    if os.path.exists(f\"{path_models}/UnivImput_NAs\"):\n",
    "        UnivImput_NAs = LoadVariable(f\"{path_models}/UnivImput_NAs\")\n",
    "    else:\n",
    "        UnivImput_NAs = SimpleImputer().fit(X_NAs)\n",
    "        SaveVariable(UnivImput_NAs, f\"{path_models}/UnivImput_NAs\")\n",
    "    X_NAs = UnivImput_NAs.transform(X_NAs)\n",
    "\n",
    "elif ImputationType == \"Multivariate\":\n",
    "    #Trying Multivariate Imputation\n",
    "    if os.path.exists(f\"{path_models}/MultivImput_NAs\"):\n",
    "        MultivImput_NAs = LoadVariable(f\"{path_models}/MultivImput_NAs\")\n",
    "    else:\n",
    "        MultivImput_NAs = IterativeImputer(random_state = RandomState, max_iter = 5, n_nearest_features = 200).fit(X_NAs)\n",
    "        SaveVariable(MultivImput_NAs, f\"{path_models}/MultivImput_NAs\")\n",
    "    X_NAs = MultivImput_NAs.transform(X_NAs)\n",
    "####################\n",
    "\n",
    "\n",
    "####################\n",
    "### Handling NAs ###\n",
    "NBeforeNADrop = len(XY)\n",
    "XY = XY.dropna()\n",
    "DroppedNARows = NBeforeNADrop - len(XY)\n",
    "if DroppedNARows > 0: print(\"Dropped NA rows count:\", DroppedNARows)\n",
    "\n",
    "if CustomNAString is not None:\n",
    "    NBeforeCustomNADrop = len(XY)\n",
    "    XY = XY.replace(CustomNAString, np.nan, regex = False).dropna()\n",
    "    DroppedCustomNARows = NBeforeCustomNADrop - len(XY)\n",
    "    if DroppedCustomNARows > 0: print(\"Dropped custom NA rows count:\", DroppedCustomNARows)\n",
    "if DroppedNARows > 0 or ((\"DroppedCustomNARows\" in locals() or \"DroppedCustomNARows\" in globals()) and DroppedCustomNARows > 0):\n",
    "    print()\n",
    "####################\n",
    "\n",
    "\n",
    "##########################################\n",
    "### Dummy Variables / One-Hot-Encoding ###\n",
    "acqCountry_DF = pd.get_dummies(XY[\"acqCountry\"].str.strip().str.lower(), prefix = 'acqCountry')\n",
    "merchantCountryCode_DF = pd.get_dummies(XY[\"merchantCountryCode\"].str.strip().str.lower(), prefix = 'merchantCountryCode')\n",
    "merchantCategoryCode_DF = pd.get_dummies(XY[\"merchantCategoryCode\"].str.strip().str.lower(), prefix = 'merchantCategoryCode')\n",
    "transactionType_DF = pd.get_dummies(XY[\"transactionType\"].str.strip().str.lower(), prefix = 'transactionType')\n",
    "XY = pd.concat([XY.drop([\"acqCountry\", \"merchantCountryCode\", \"merchantCategoryCode\", \"transactionType\"], axis = 1), acqCountry_DF, merchantCountryCode_DF, merchantCategoryCode_DF, transactionType_DF], axis = 1)\n",
    "##########################################\n",
    "\n",
    "\n",
    "##########################################\n",
    "### Keeping only the Variables we need ###\n",
    "DependentVarName = \"isFraud\"\n",
    "OtherDependentVarName = list(set([]) - set([DependentVarName]))\n",
    "# accountNumber #Irrelevant\n",
    "# customerId #Irrelevant\n",
    "# transactionDateTime #Useless (by its own)\n",
    "# merchantName #Too many levels\n",
    "# currentExpDate #Useless (by its own)\n",
    "# accountOpenDate #Useless (by its own)\n",
    "# dateOfLastAddressChange #Useless (by its own)\n",
    "# cardCVV #Irrelevant\n",
    "# enteredCVV #Irrelevant\n",
    "# cardLast4Digits #Irrelevant\n",
    "OtherVariablesToBeDropped = [\"accountNumber\", \"customerId\", \"transactionDateTime\", \"merchantName\", \"currentExpDate\", \"accountOpenDate\", \"dateOfLastAddressChange\", \"cardCVV\", \"enteredCVV\", \"cardLast4Digits\"]\n",
    "ColumnsToBeDropped = list(set([DependentVarName] + OtherDependentVarName + OtherVariablesToBeDropped))\n",
    "ColumnsToBeDropped = [Col for Col in ColumnsToBeDropped if (Col in XY.columns.values)]\n",
    "ColumnsToKeepX = XY.columns.values[~pd.Series(XY.columns.values).isin(ColumnsToBeDropped).values]\n",
    "ColumnsToKeepY = XY.columns.values[XY.columns.values == DependentVarName]\n",
    "##########################################\n",
    "\n",
    "\n",
    "################################\n",
    "### Getting a Train/Test set ###\n",
    "X = XY.loc[:, ColumnsToKeepX].values.astype(np.float32)\n",
    "Y = XY.loc[:, ColumnsToKeepY].values.astype(np.float32)\n",
    "if len(Y.shape) == 2 and Y.shape[1] == 1: #This is univariate, so let's get a vector for Y instead of a matrix\n",
    "    Y = Y.squeeze()\n",
    "\n",
    "N = X.shape[0]\n",
    "\n",
    "TrainPerc = 0.8\n",
    "#==Stratified Split\n",
    "init_seeds(Seed)\n",
    "TrainIndx, TestIndx = train_test_split(np.arange(X.shape[0]), test_size = 1 - TrainPerc, shuffle = True, stratify = Y, random_state = Seed)\n",
    "X_Train = X[TrainIndx]\n",
    "Y_Train = Y[TrainIndx]\n",
    "X_Test = X[TestIndx]\n",
    "Y_Test = Y[TestIndx]\n",
    "################################\n",
    "\n",
    "\n",
    "########################\n",
    "### Scaling the Data ###\n",
    "if os.path.exists(f\"{path_models}/scaler\"):\n",
    "    print(\"!!\\n!! Using saved scaler.\\n!!\\n\")\n",
    "    scaler = LoadVariable(f\"{path_models}/scaler\")\n",
    "else:\n",
    "    scaler = StandardScaler(with_mean = True, with_std = True).fit(X_Train)\n",
    "    SaveVariable(scaler, f\"{path_models}/scaler\")\n",
    "    \n",
    "X_Train = scaler.transform(X_Train)\n",
    "X_Test = scaler.transform(X_Test)\n",
    "########################\n",
    "\n",
    "\n",
    "##########################################\n",
    "### Taking care of the Class Imbalance ###\n",
    "X_Train = np.concatenate((X_Train, np.tile(X_Train[Y_Train == 1], (20, 1))))\n",
    "Y_Train = np.concatenate((Y_Train, np.tile(Y_Train[Y_Train == 1], (20))))\n",
    "##########################################\n",
    "\n",
    "\n",
    "###################################\n",
    "### Creating Dataset/Dataloader ###\n",
    "TrainDataset = TensorDataset(torch.from_numpy(X_Train), torch.from_numpy(Y_Train))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = TrainDataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    pin_memory = False\n",
    ")\n",
    "\n",
    "TestDataset = TensorDataset(torch.from_numpy(X_Test), torch.from_numpy(Y_Test))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = TestDataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    pin_memory = False\n",
    ")\n",
    "###################################\n",
    "\n",
    "K, NTrain, D, H1, W1 = (1, 812794, 38, 0, 0)\n",
    "\n",
    "print(\"\\n{DependentVarName}\")\n",
    "sns.countplot(x = DependentVarName, data = XY)\n",
    "plt.show()\n",
    "\n",
    "XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgfZM1R4fm7w"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "05fNyIgDfm7x"
   },
   "source": [
    "## Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "iozXGB-ffm7x"
   },
   "source": [
    "●\tEach of the transactions in the dataset has a field called isFraud. Please build a predictive model to determine whether a given transaction will be fraudulent or not. Use as much of the data as you like (or all of it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "rMEzwgqcfm7y"
   },
   "source": [
    "### Non Dynamic (Static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "EsR0zDEyfm7y",
    "outputId": "8b1a73b3-6c49-4f84-e036-5151f68adad9"
   },
   "outputs": [],
   "source": [
    "#Static\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, K, num_units, dropout, batchnorm_momentum):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=num_units[0], out_features=num_units[1], bias=usebias[0]),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(num_units[1], eps=1e-05, momentum=batchnorm_momentum[0], affine=True, track_running_stats=True),\n",
    "            nn.Dropout(p=dropout[0], inplace=False),\n",
    "            nn.Linear(in_features=num_units[1], out_features=num_units[2], bias=usebias[1]),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(num_units[2], eps=1e-05, momentum=batchnorm_momentum[1], affine=True, track_running_stats=True),\n",
    "            nn.Dropout(p=dropout[1], inplace=False),\n",
    "            nn.Linear(in_features=num_units[2], out_features=num_units[3], bias=usebias[2]),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(num_units[3], eps=1e-05, momentum=batchnorm_momentum[2], affine=True, track_running_stats=True),\n",
    "            nn.Dropout(p=dropout[2], inplace=False),\n",
    "            nn.Linear(in_features=num_units[3], out_features=num_units[4], bias=usebias[3]),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(num_units[4], eps=1e-05, momentum=batchnorm_momentum[3], affine=True, track_running_stats=True),\n",
    "            nn.Dropout(p=dropout[3], inplace=False),\n",
    "            nn.Linear(in_features=num_units[4], out_features=num_units[5], bias=usebias[4]),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(num_units[5], eps=1e-05, momentum=batchnorm_momentum[4], affine=True, track_running_stats=True),\n",
    "            nn.Dropout(p=dropout[4], inplace=False),\n",
    "            nn.Linear(in_features=num_units[5], out_features=K, bias=usebias[5])\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers[0](x)        \n",
    "        out = self.layers[1](out)\n",
    "        out = self.layers[2](out)\n",
    "        out = self.layers[3](out)\n",
    "        out = self.layers[4](out)\n",
    "        out = self.layers[5](out)\n",
    "        out = self.layers[6](out)\n",
    "        out = self.layers[7](out)\n",
    "        out = self.layers[8](out)\n",
    "        out = self.layers[9](out)    \n",
    "        out = self.layers[10](out)\n",
    "        out = self.layers[11](out)\n",
    "        out = self.layers[12](out)\n",
    "        out = self.layers[13](out)\n",
    "        out = self.layers[14](out)\n",
    "        out = self.layers[15](out)\n",
    "        out = self.layers[16](out)\n",
    "        out = self.layers[17](out)\n",
    "        out = self.layers[18](out)\n",
    "        out = self.layers[19](out)\n",
    "        out = self.layers[20](out)\n",
    "        \n",
    "        return out\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3Oi6WBqfm7y"
   },
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABmEPTRTfm7y",
    "outputId": "1d1465fe-ac4f-484d-97d6-4c3f34e8ae16"
   },
   "outputs": [],
   "source": [
    "conv_input_size = X_Train[0].shape if X_Train is not None else X[0].shape\n",
    "input_size = np.prod(conv_input_size)\n",
    "output_size = K\n",
    "hn1 = D\n",
    "print(\"conv_input_size: \" + str(conv_input_size) + \", input_size: \" + str(input_size) + \", D: \" + str(D) + \", output_size: \" + str(output_size))\n",
    "\n",
    "ReluAlpha = 0.0 #0.01 def leakyRelu\n",
    "EluAlpha = 0.8\n",
    "\n",
    "layer_type = ['dense', 'dense', 'dense', 'dense', 'dense'] #\"dense\", \"rnn\", \"gru\", \"lstm\", \"conv\", \"stridedconv\", \"convpool\"\n",
    "###\n",
    "NUM = 1\n",
    "num_units = [hn1, 50, 50, 10, 25, 25]\n",
    "num_units = [num_units[0], *[n_unit * NUM for n_unit in num_units[1:]]]\n",
    "###\n",
    "activation = [\"tanh\"] + [\"tanh\"] * (len(layer_type)-1) #Avoid \"relu\" on RNN #None, \"relu6\" \"relu\", \"elu\", \"softplus\", \"tanh\", \"sigmoid\" #For RNNs (LSTM etc) tanh or relu only [check compatibility]\n",
    "###\n",
    "dropout    = [0.3] + [0.3] * (len(layer_type)-1) #Might be a bad idea on CNN as we're trying to find patterns\n",
    "###\n",
    "batchnorm = [True] * len(layer_type) #DOESNT WORK with Multistep-forecast (batch of 1) #Batchnorm already does regularization, so we usually don't need to add dropout as well\n",
    "batchnorm_momentum = [0.1] * len(layer_type) #0.99 default Tensorflow, 0.1 Pytorch #used for the running_mean and running_var computation\n",
    "###\n",
    "usebias = [not batchnormlayer for batchnormlayer in batchnorm] + [True] #Length +1 because of the Output layer\n",
    "#usebias = [True] * len(layer_type) + [True] #For CNN perhaps we want to have both BatchNorm + a Bias\n",
    "###\n",
    "l2_lamda = 0.005\n",
    "mu = 0.8 #Momentum\n",
    "print()\n",
    "print(\"num_units\", num_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rSU47g3fm7y"
   },
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSzyk35ffm7y"
   },
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8iac1_7fm7y",
    "outputId": "f77dfc46-904a-4916-f152-4c43e52fc4ed"
   },
   "outputs": [],
   "source": [
    "model = Net(K, num_units, dropout, batchnorm_momentum).to(device)\n",
    "print(device)\n",
    "print(model)\n",
    "\n",
    "train_losses = np.array([])\n",
    "test_losses = np.array([])\n",
    "train_best_loss = np.Inf\n",
    "test_best_loss = np.Inf\n",
    "Metric1 = 0 #Initialising as the worst possible value\n",
    "Metric2 = np.nan\n",
    "Metric3 = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTkInP0sfm7z",
    "outputId": "0c39a58f-91de-41fb-c351-4250559a61e3"
   },
   "outputs": [],
   "source": [
    "summary(model, input_size = tuple([2, *conv_input_size]), device = device, verbose = 2, col_names = [\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxThNvxsfm7z",
    "outputId": "df52331b-3bf1-4bf5-a3e1-4762751949e4"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss() #Using Binary Cross Entropy loss function\n",
    "print(\"Binary Classification\")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr = learning_rate, betas = (mu, 0.999), weight_decay = l2_lamda, amsgrad = False) #LR=0.001 (0.9, 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "LQ5IvIZffm7z"
   },
   "source": [
    "### Prerequisite Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "29n2-Wnsfm7z"
   },
   "outputs": [],
   "source": [
    "def AUCCalculation(Targets, Y_Prob, Y_Hat, K):\n",
    "    if isinstance(Targets, torch.Tensor):\n",
    "        Targets = Targets.cpu().numpy()\n",
    "        \n",
    "    if isinstance(Y_Prob, torch.Tensor):\n",
    "        Y_Prob = Y_Prob.cpu().numpy()\n",
    "        \n",
    "    if isinstance(Y_Hat, torch.Tensor):\n",
    "        Y_Hat = Y_Hat.cpu().numpy()\n",
    "    \n",
    "    if K == 1:\n",
    "        try:\n",
    "            CurMetric2 = roc_auc_score(Targets, Y_Prob) #Calculating AUC #Cares for performance both in Positives and Negatives (but may not fare well with heavy class imbalance)\n",
    "        except Exception as exc:\n",
    "            CurMetric2 = np.nan\n",
    "            warnings.warn(f\"\\nAn error occurred in AUC calculation (probably because the random batch of data includes only 1 of the 2 classes?).\\nThe error reads: {exc}\")                    \n",
    "            print(\"set(Targets): \", list(set(Targets.reshape(-1))), \"set(Outputs): \", list(set(Y_Hat.reshape(-1))))\n",
    "    else:\n",
    "        try:\n",
    "            CurMetric2 = roc_auc_score(Targets, Y_Prob, multi_class = \"ovr\", average = 'weighted') #Calculating Weighted AUC #Cares for performance both in Positives and Negatives (but may not fare well with heavy class imbalance)\n",
    "        except Exception as exc:\n",
    "            CurMetric2 = np.nan\n",
    "            warnings.warn(f\"\\nAn error occurred in AUC calculation (probably because of missing classes in the random batch of data?).\\nThe error reads: {exc}\")\n",
    "            print(\"set(Targets): \", list(set(Targets.reshape(-1))), \"set(Outputs): \", list(set(Y_Hat.reshape(-1))))\n",
    "            \n",
    "    return CurMetric2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "7heOSccGfm7z"
   },
   "outputs": [],
   "source": [
    "def F1ScoreCalculation(Targets, Y_Hat, K):\n",
    "    if K == 1:\n",
    "        try:\n",
    "            CurMetric3 = f1_score(Targets.cpu().numpy(), Y_Hat.cpu().numpy()) #Calculating F1 #Cares about balance between Precision and Recall (Sensitivity)\n",
    "        except Exception as exc:\n",
    "            CurMetric3 = np.nan\n",
    "            warnings.warn(f\"\\nAn error occurred in F1 score calculation (probably because the random batch of data includes only 1 of the 2 classes?).\\nThe error reads: {exc}\")\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            CurMetric3 = f1_score(Targets.cpu().numpy(), Y_Hat.cpu().numpy(), average = 'weighted') #Calculating Weighted F1 #Cares about balance between Precision and Recall (Sensitivity)\n",
    "        except Exception as exc:\n",
    "            CurMetric3 = np.nan\n",
    "            warnings.warn(f\"\\nAn error occurred in F1 score calculation (probably because of missing classes in the random batch of data?).\\nThe error reads: {exc}\")\n",
    "            \n",
    "    return CurMetric3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "o4tf2TRUfm7z"
   },
   "outputs": [],
   "source": [
    "def PrintIterationMetrics(it, epochs, t0, train_loss, test_loss, first_metric, first_metric_Name, second_metric, second_metric_Name, third_metric, third_metric_Name, MaxTrainLossLen, MaxTestLossLen, MaxMetric1Len, MaxMetric2Len, MaxMetric3Len):\n",
    "    dt = datetime.now() - t0\n",
    "\n",
    "    strTrainLoss = f\"{train_loss:.4f}\"\n",
    "    strTestLoss = f\"{test_loss:.4f}\"\n",
    "    strMetric1 = f'{first_metric:.3f}'\n",
    "    strMetric2 = f'{second_metric:.3f}'\n",
    "    strMetric3 = f'{third_metric:.3f}'\n",
    "    if it == 0:\n",
    "        MaxTrainLossLen = len(strTrainLoss)\n",
    "        MaxTestLossLen = len(strTestLoss)\n",
    "        MaxMetric1Len = len(strMetric1)\n",
    "        MaxMetric2Len = len(strMetric2)\n",
    "        MaxMetric3Len = len(strMetric3)\n",
    "    print(f'Epoch {ZeroANumber(it+1, len(str(epochs)))}/{epochs}, Train Loss: {SpaceAString(strTrainLoss, MaxTrainLossLen)}, Test Loss: {SpaceAString(strTestLoss, MaxTestLossLen)}, {first_metric_Name}: {SpaceAString(strMetric1, MaxMetric1Len)}, {second_metric_Name}: {SpaceAString(strMetric2, MaxMetric1Len)}, {third_metric_Name}: {SpaceAString(strMetric3, MaxMetric1Len)}, Duration: {dt}')\n",
    "    return MaxTrainLossLen, MaxTestLossLen, MaxMetric1Len, MaxMetric2Len, MaxMetric3Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "HFOms5k1fm70"
   },
   "outputs": [],
   "source": [
    "def UpdateMetricsAndSaveModel(model, train_loss, test_loss, train_best_loss, test_best_loss, CurMetric1, Metric1, CurMetric2, Metric2, CurMetric3, Metric3):\n",
    "    if (test_loss < test_best_loss): #Saving the model if it outperforms previous iteration's model\n",
    "        test_best_loss = test_loss\n",
    "        train_best_loss = train_loss\n",
    "        torch.save(model.state_dict(), f\"model_dict.pt\") #Saving Model's Dictionary\n",
    "        \n",
    "        if np.isfinite(CurMetric1) and CurMetric1 >= Metric1:\n",
    "            Metric1 = CurMetric1\n",
    "            Metric2 = CurMetric2\n",
    "            Metric3 = CurMetric3\n",
    "            torch.save(model.state_dict(), f\"acc_model_dict.pt\") #Saving Model's Dictionary\n",
    "    return train_best_loss, test_best_loss, Metric1, Metric2, Metric3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "IJzfwdRgfm70"
   },
   "outputs": [],
   "source": [
    "def PrintFinishingInformation(start_time, JustCalculateElapsedTime = False):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if not JustCalculateElapsedTime:\n",
    "        FinishedOn = datetime.now(timezone('Europe/Athens')).strftime(\"%a, %Y-%m-%d %H:%M %Z %z\")\n",
    "        print(\"\\nDone (\" + FinishedOn + \") Elapsed time: \" + str(round(elapsed_time, 1)) + \" seconds\")\n",
    "    \n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "e907JXIHfm70"
   },
   "outputs": [],
   "source": [
    "def TrainModel(model, optimiser, criterion, X_Train, Y_Train, K):\n",
    "    model.train() #Putting model in training mode so that things like dropout() are activated again\n",
    "    \n",
    "    optimiser.zero_grad() #Initialisation of the gradient of θ\n",
    "    outputs = model(X_Train) #Getting the prediction using the forward direction of the Neural Net\n",
    "\n",
    "    if K == 1:\n",
    "        outputs = outputs.view(-1) #Target is a vector, so let's ensure predictions are a vector too\n",
    "    \n",
    "    loss = criterion(outputs, Y_Train) #Calculating the loss according to the loss function\n",
    "    loss.backward() #Calculating the Gradient Δθ of the loss function with respect to the parameters\n",
    "    \n",
    "    optimiser.step() #Calculates and updates the parameters θ using gradient descent, as θ = θ - η*Δθ\n",
    "    \n",
    "    return optimiser, outputs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "di3Cq2TPfm70"
   },
   "outputs": [],
   "source": [
    "def EvaluateModel(model, criterion, Inputs, Targets, K, ScalerToInverseTransform = None, numPredictors = len(ColumnsToKeepY)):\n",
    "    model.eval() #Putting model in evaluation mode so that things like dropout() are deactivated\n",
    "    with torch.no_grad(): #Making sure that we don't update the gradient outside the training part\n",
    "        Y_Prob = model(Inputs) #Getting the prediction using the forward direction of the Neural Net\n",
    "        \n",
    "        if K == 1:\n",
    "            Y_Prob = Y_Prob.view(-1) #Target is a vector, so let's ensure predictions are a vector too\n",
    "\n",
    "        loss_scalar = criterion(Y_Prob, Targets).item() #Calculating the loss according to the loss function\n",
    "\n",
    "        if K == 1:\n",
    "            Y_Prob = nn.Sigmoid()(Y_Prob) #The loss function includes the sigmoid so we need to use it here to get the probabilities\n",
    "            Y_Hat = (Y_Prob >= 0.5) #Y_Hat is whether or not the probability is greater than the threshold            \n",
    "        else:\n",
    "            Y_Prob = nn.Softmax(dim = 1)(Y_Prob) #dim: every slice along dim will sum to 1\n",
    "            _, Y_Hat = torch.max(Y_Prob, 1) #Prediction. torch.max returns both max (value) and argmax (index)\n",
    "\n",
    "        CurMetric1 = (Y_Hat == Targets).float().mean().cpu().numpy().squeeze() #Calculating Accuracy\n",
    "        CurMetric2 = AUCCalculation(Targets, Y_Prob, Y_Hat, K)\n",
    "        CurMetric3 = F1ScoreCalculation(Targets, Y_Hat, K)\n",
    "            \n",
    "        return Y_Prob, Y_Hat, loss_scalar, CurMetric1, CurMetric2, CurMetric3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "TYQKOVkBfm70"
   },
   "outputs": [],
   "source": [
    "def FixFormatAndDTypes(device, H1, W1, D, K, Inputs, Targets):\n",
    "    if isinstance(Inputs, np.ndarray):\n",
    "        Inputs = torch.from_numpy(Inputs)\n",
    "    if isinstance(Targets, np.ndarray):\n",
    "        Targets = torch.from_numpy(Targets)\n",
    "    \n",
    "    Inputs = Inputs.to(device)\n",
    "    Targets = Targets.to(device)\n",
    "    \n",
    "    Inputs = Inputs.float()\n",
    "    Targets = Targets.float() #For Binary Classification: Even though it technically is an integer, float calculations will take place, so making it a float\n",
    "    \n",
    "    return Inputs, Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tGkoJtQfm70"
   },
   "source": [
    "### Stochastic Gradient Descent (Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "vB2eoeWefm70"
   },
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "vpOOEF3dfm70"
   },
   "outputs": [],
   "source": [
    "def batch_gd(model, criterion, optimiser, scheduler, train_loader, test_loader, epochs, PrintInfoEverynEpochs):\n",
    "    global train_best_loss #Assigning to globals()\n",
    "    global test_best_loss #Assigning to globals()\n",
    "    global Metric1 #Assigning to globals() #Accuracy for Classification or R2 for Regression\n",
    "    global Metric2 #Assigning to globals() #AUC for Classification or Adj.R2 for Regression\n",
    "    global Metric3 #Assigning to globals() #F1 Score for Classification or MAE for Regression\n",
    "    MaxTrainLossLen, MaxTestLossLen, MaxMetric1Len, MaxMetric2Len, MaxMetric3Len = None, None, None, None, None #For output text formatting\n",
    "    \n",
    "    start_time = time.time() #To calculate the duration of the whole learning procedure\n",
    "    model = model.to(device) #If there is a GPU, let's ensure model is sent to the GPU\n",
    "    \n",
    "    train_losses = np.zeros(epochs) #Initialising the losses\n",
    "    test_losses = np.zeros(epochs)  #Initialising the losses\n",
    "    for it in range(epochs):\n",
    "        t0 = datetime.now() #To calculate the duration of the current epoch\n",
    "        train_loss = [] #Initialising the loss for current epoch\n",
    "        train_weights = []\n",
    "        \n",
    "        #== Training ==#\n",
    "        #for inputs, targets in train_loader:\n",
    "        for inputs, targets in tqdm(train_loader, total = len(train_loader), leave = False):\n",
    "            inputs, targets = FixFormatAndDTypes(device, H1, W1, D, K, inputs, targets) #Making sure we have Tensors of the correct Format and Data Type\n",
    "            optimiser, outputs, loss = TrainModel(model, optimiser, criterion, inputs, targets, K) #Training the model on Train set\n",
    "            train_loss.append(loss.item())\n",
    "            train_weights.append(targets.shape[0])\n",
    "        train_loss = np.average(train_loss, weights = train_weights) #Weighted average based on number of samples #Alternatively we could add all Iteration Losses instead of 1 Epoch loss, but the array length will be higher than Validation's\n",
    "        \n",
    "        #== Evaluation ==#\n",
    "        test_loss = []\n",
    "        test_metric1 = []\n",
    "        test_metric2 = []\n",
    "        test_metric3 = []\n",
    "        test_weights = []\n",
    "        #for inputs, targets in test_loader:\n",
    "        for inputs, targets in tqdm(test_loader, total = len(test_loader), leave = False):\n",
    "            inputs, targets = FixFormatAndDTypes(device, H1, W1, D, K, inputs, targets) #Making sure we have Tensors of the correct Format and Data Type\n",
    "            _, _, cur_test_loss, cur_test_metric1, cur_test_metric2, cur_test_metric3 = EvaluateModel(model, criterion, inputs, targets, K) #Evaluating the model on Evaluation set\n",
    "            test_loss.append(cur_test_loss)\n",
    "            test_metric1.append(cur_test_metric1)\n",
    "            test_metric2.append(cur_test_metric2)\n",
    "            test_metric3.append(cur_test_metric3)\n",
    "            test_weights.append(targets.shape[0])\n",
    "        test_loss = np.average(test_loss, weights = test_weights) #Weighted average based on number of samples\n",
    "        CurMetric1 = np.average(test_metric1, weights = test_weights) #Weighted average based on number of samples\n",
    "        CurMetric2 = np.average(test_metric2, weights = test_weights) #Weighted average based on number of samples\n",
    "        CurMetric3 = np.average(test_metric3, weights = test_weights) #Weighted average based on number of samples\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "        \n",
    "        if (it + 1) % PrintInfoEverynEpochs == 0 or it == 0: #Printing information about the Loss and metric\n",
    "            MaxTrainLossLen, MaxTestLossLen, MaxMetric1Len, MaxMetric2Len, MaxMetric3Len = PrintIterationMetrics( #Prints Iteration Metrics\n",
    "                it, epochs, t0, train_loss, test_loss,\n",
    "                CurMetric1, \"Acc\",\n",
    "                CurMetric2, \"AUC\",\n",
    "                CurMetric3, \"F1\",\n",
    "                MaxTrainLossLen, MaxTestLossLen,\n",
    "                MaxMetric1Len, MaxMetric2Len, MaxMetric3Len\n",
    "            )\n",
    "        \n",
    "        train_best_loss, test_best_loss, Metric1, Metric2, Metric3 = UpdateMetricsAndSaveModel(model, train_loss, test_loss, train_best_loss, test_best_loss, CurMetric1, Metric1, CurMetric2, Metric2, CurMetric3, Metric3) #Updating Metrics and Saving the model if it outperforms previous iteration's model\n",
    "    \n",
    "    elapsed_time = PrintFinishingInformation(start_time) #Prints finishing information\n",
    "    return train_losses, test_losses, train_best_loss, test_best_loss, Metric1, Metric2, Metric3, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmY76r88fm71"
   },
   "source": [
    "#### Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2q3RtmGfm71",
    "outputId": "75d1aa3e-b0df-4ea3-9e78-102552bad776"
   },
   "outputs": [],
   "source": [
    "GDType = \"stochastic\"\n",
    "Epochs = 10\n",
    "PrintInfoEverynEpochs = 1\n",
    "\n",
    "scheduler = None\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size = Epochs // 10 if Epochs > 10 else 3, gamma = 0.8)\n",
    "\n",
    "new_train_losses, new_test_losses, train_best_loss, test_best_loss, Metric1, Metric2, Metric3, elapsed_time = batch_gd(model, criterion, optimiser, scheduler, train_loader, test_loader, epochs = Epochs, PrintInfoEverynEpochs = PrintInfoEverynEpochs)\n",
    "train_losses = np.append(train_losses, new_train_losses)\n",
    "test_losses = np.append(test_losses, new_test_losses)\n",
    "train_loss = train_losses[-1]\n",
    "test_loss = test_losses[-1]\n",
    "print(\"\\ntrain_best_loss:\", train_best_loss, \"test_best_loss:\", test_best_loss, \"Acc:\", Metric1, \"AUC:\", Metric2, \"F1:\", Metric3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFvIOVPbfm71"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model_dict.pt\"))\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5E277SJfm71"
   },
   "outputs": [],
   "source": [
    "train_losses = np.array([])\n",
    "test_losses = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__9U-dsvfm71"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-TyBCcsfm71"
   },
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "LLojROmwfm71",
    "outputId": "fee9b7d1-e0bd-47e6-e410-c948bacf5c5d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label = 'train loss')\n",
    "plt.plot(test_losses, label = 'test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zt0iNhYqfm71"
   },
   "source": [
    "●\tProvide an estimate of performance using an appropriate sample, and show your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "B4p6RPlXfm71",
    "outputId": "e6a23db8-f8cd-4699-d1b5-119d50b4410f"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Actual_Y = np.array([])\n",
    "    Y_Prob = np.array([])\n",
    "    Y_Hat = np.array([])\n",
    "\n",
    "    Weights = []\n",
    "    train_loss = []\n",
    "    train_Acc = []\n",
    "    train_AUC = []\n",
    "    train_F1s = []\n",
    "\n",
    "    print(\"Stochastic GD\")\n",
    "    for inputs, targets in tqdm(train_loader, total = len(train_loader), leave = False):\n",
    "        inputs, targets = FixFormatAndDTypes(device, H1, W1, D, K, inputs, targets)\n",
    "        Actual_Y = np.append(Actual_Y, targets.cpu().numpy(), axis = 0)\n",
    "\n",
    "        cur_Y_Prob, cur_Y_Hat, cur_train_loss, cur_train_Acc, cur_train_AUC, cur_train_F1 = EvaluateModel(model, criterion, inputs, targets, K)\n",
    "        Y_Prob = np.append(Y_Prob, cur_Y_Prob.cpu().numpy(), axis = 0) #Prediction probability.\n",
    "        Y_Hat = np.append(Y_Hat, cur_Y_Hat.cpu().numpy(), axis = 0) #Prediction.\n",
    "\n",
    "        Weights.append(cur_Y_Prob.shape[0])\n",
    "        train_loss.append(cur_train_loss)\n",
    "        train_Acc.append(cur_train_Acc)\n",
    "        train_AUC.append(cur_train_AUC)\n",
    "        train_F1s.append(cur_train_F1)\n",
    "\n",
    "    train_loss = np.average(train_loss, weights = Weights)\n",
    "    train_Acc = np.average(train_Acc, weights = Weights)\n",
    "    train_AUC = np.average(train_AUC, weights = Weights)\n",
    "    train_F1s = np.average(train_F1s, weights = Weights)\n",
    "\n",
    "    Test_Actual_Y = np.array([])\n",
    "    Pred = np.array([])\n",
    "    test_loss = []\n",
    "    for inputs, targets in tqdm(test_loader, total = len(test_loader), leave = False):\n",
    "        inputs, targets = FixFormatAndDTypes(device, H1, W1, D, K, inputs, targets)\n",
    "\n",
    "        cur_Y_Prob, cur_Y_Hat, cur_train_loss, cur_train_Acc, cur_train_AUC, cur_train_F1 = EvaluateModel(model, criterion, inputs, targets, K)\n",
    "\n",
    "        Test_Actual_Y = np.append(Test_Actual_Y, targets.cpu().numpy(), axis = 0)\n",
    "        outputs = model(inputs).view(-1,) #Getting the prediction using the forward direction of the Neural Net\n",
    "        CurLoss = criterion(outputs, targets) #Calculating the loss according to the loss function\n",
    "        test_loss.append(CurLoss.item())\n",
    "        outputs = outputs.detach().cpu().numpy().squeeze()\n",
    "        outputs = nn.Sigmoid()(torch.from_numpy(outputs)).numpy()\n",
    "        Pred = np.append(Pred, outputs, axis = 0) #Prediction.\n",
    "\n",
    "    test_loss = np.mean(test_loss)\n",
    "    test_Acc = np.mean((Pred >= 0.5) == Test_Actual_Y)\n",
    "    test_AUC = roc_auc_score(Test_Actual_Y, Pred)\n",
    "    test_F1 = f1_score(Test_Actual_Y, np.round(Pred, 0))\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.3f}. Acc: {(train_Acc * 100.):.2f}%. AUC: {train_AUC:.3f}. F1: {train_F1s:.3f}\")\n",
    "    print(f\"Test  loss: {test_loss:.3f}. Acc: {(test_Acc * 100.):.2f}%. AUC: {test_AUC:.3f}. F1: {test_F1:.3f}\")\n",
    "    print()\n",
    "\n",
    "    #Confusion Matrix\n",
    "    cm = confusion_matrix(Test_Actual_Y, Pred >= 0.5) #This should not use Y_Test on Stochastic\n",
    "    plot_confusion_matrix(cm, [False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gKpbLs5fm72"
   },
   "source": [
    "●\tPlease explain your methodology (modeling algorithm/method used and why, what features/data you found useful, what questions you have, and what you would do next with more time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vThEmhjJfm72"
   },
   "outputs": [],
   "source": [
    "#The first step was to transform the data from an unstructured JSON format into a structured tabular format that Machine Learning algorithms can understand.\n",
    "#\n",
    "#Preprocessing included things like transforming string fields representing datetimes into acutal datetime fields, replacing missing values encoded as emptry strings with actual missing values (NaNs), and so on.\n",
    "#There is code for univariate and multivariate imputation, as well as replacing with 0s, controlled by the value of the variable \"ImputationType\". In this iteration no imputation was used, however and rows with missing values were removed since there is a vast amount of data.\n",
    "#Categorical variables were one-hot-encoded, and these variables were removed since they were completely empty: echoBuffer, merchantCity, merchantState, merchantZip, posOnPremises, recurringAuthInd.\n",
    "#\n",
    "#merchantName simply has too many levels to be of any use, so it's been discarded.\n",
    "#accountNumber, customerId, cardCVV, enteredCVV and cardLast4Digits are irrelevant and they change by person (many times they even change by purchase even for the same person), so they too were dropped.\n",
    "#transactionDateTime, currentExpDate, accountOpenDate and dateOfLastAddressChange are dates and are useless for machine learning. At least in their current form. They can be used to feature engineering, however. For instance, engineering a feature of seconds passed since last purchase.\n",
    "#The rest of the variables were used on the model\n",
    "#\n",
    "#The Training/Testing set splitting was done in a stratified manner on account of the significant class-imbalance of the dataset. There is a 80% training set and 20% testing set.\n",
    "#On the training set, there is also heavy oversampling of the minority class, so as to aid the training part to recognise it better.\n",
    "#The dataset is also normalised before passed to the model, transformed into a PyTorch dataframe and subsequently a data loader is created to yield batches of it in a random fashion.\n",
    "#\n",
    "#The model architecture is an instance of a Deep Feed-Forward Neural Network with Tanh activation functions, dropout regularization to avoid overfitting, batch-normalization which also aids in regularisation as well,\n",
    "#    and 1 neuron with a sigmoid activation at the end to produce the probability of the class being the positive one. It should be noted, in PyTorch, the sigmoid is part of the loss function, which is why it's not included in the code.\n",
    "#The loss function is the Binary Cross Entropy, and the optimiser is the AdamW variant of Stochastic Gradient Descent. There is also an L2 regularization to help with overfitting.\n",
    "#\n",
    "#The cost is plotted after training so we can observe its behaviour and tweak hyperparameters accordingly, and the evaluation includes the Accuracy (which is somewhat meaningless for class-imbalanced datasets), the Area Under the ROC Curve, and the F1 score.\n",
    "#There is also a confusion matrix which shows the exact number of True Positives/Negative and False Positives/Negatives.\n",
    "#\n",
    "#Lastly, there is a section for Saving and Loading the trained model, as well as visualising it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "QbTYGKy3fm72"
   },
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "JiSzFMHYfm72",
    "outputId": "052577a4-350e-438c-8240-3d6d6e75c6af"
   },
   "outputs": [],
   "source": [
    "print(\"path_root:\", path_root, \"\\n\")\n",
    "\n",
    "SaveFolder = (f\"{path_root}/Models/\" +\n",
    "              datetime.now(timezone('Europe/Athens')).strftime(\"%Y-%m-%d %H-%M\") + \", \" +\n",
    "              \"loss \" + \"{0:.2f}\".format(test_loss) + \", \" +\n",
    "              \"Acc \" + \"{0:.2f}\".format(Metric1) + \", \" +\n",
    "              \"AUC \" + \"{0:.2f}\".format(Metric2)\n",
    ")\n",
    "print(SaveFolder)\n",
    "os.makedirs(SaveFolder, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "dGuCWJPCfm72"
   },
   "outputs": [],
   "source": [
    "### Saving the Model ###\n",
    "torch.save(model.state_dict(), SaveFolder + \"/model_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "Ssq4jC1mfm72"
   },
   "source": [
    "# Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "rOZsC5Abfm72",
    "outputId": "541f957b-1697-4443-c241-86e4de69ec89"
   },
   "outputs": [],
   "source": [
    "SaveFolder = fr\"{path_root}\\Models\\2022-4-22 12-57, loss 0.26, Acc 0.92, AUC 0.78\"\n",
    "print(f\"Using explicitly defined SaveFolder = {SaveFolder}\")\n",
    "\n",
    "model = Net(K, num_units, dropout, batchnorm_momentum).to(device)\n",
    "#model.load_state_dict(torch.load(SaveFolder + \"/model_dict.pt\"))\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "8fZ3ftEYfm72"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "hidden": true,
    "id": "8wJivhLjfm73",
    "outputId": "4c934efd-bce0-4021-bddf-230bfb69cefb"
   },
   "outputs": [],
   "source": [
    "#conda install graphviz python-graphviz\n",
    "!pip install hiddenlayer\n",
    "import hiddenlayer as hl\n",
    "if \"TrainDataset\" in locals() or \"TrainDataset\" in globals():\n",
    "    hl_graph = hl.build_graph(model, torch.zeros(list([2, *conv_input_size]), device = device), )\n",
    "else:\n",
    "    hl_graph = hl.build_graph(model, torch.zeros(list(conv_input_size), device = device), )\n",
    "hl_graph.theme = hl.graph.THEMES[\"blue\"].copy()\n",
    "display(hl_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "5UyNwR7wfm73",
    "outputId": "93b35efd-713f-46d7-bbc1-7b5b7eeb438d"
   },
   "outputs": [],
   "source": [
    "!pip install torchviz\n",
    "from torchviz import make_dot\n",
    "if \"TrainDataset\" in locals() or \"TrainDataset\" in globals():\n",
    "    display(make_dot(model(next(iter(TrainDataset))[0][np.newaxis].float().to(device)), params = dict(list(model.named_parameters())), show_attrs = False, show_saved = False)) #.render(\"NN\", format = \"png\")\n",
    "else:\n",
    "    display(make_dot(model(torch.from_numpy(X_Test).to(device)), params = dict(list(model.named_parameters())), show_attrs = False, show_saved = False)) #.render(\"NN\", format = \"png\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Capital One PhD Data Challenge",
   "provenance": []
  },
  "interpreter": {
   "hash": "20c5c10183822f179657c1f791f0ae50c4ae59d37c820e4f7d221afff8fe1e53"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
